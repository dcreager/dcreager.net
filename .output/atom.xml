<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://dcreager.net/</id>
  <title>dcreager.net</title>
  <updated>2016-11-17T14:00:00Z</updated>
  <link rel="alternate" href="http://dcreager.net/"/>
  <link rel="self" href="http://dcreager.net/atom.xml"/>
  <author>
    <name>Douglas Creager</name>
    <uri>http://dcreager.net/</uri>
  </author>
  <entry>
    <id>tag:dcreager.net,2016-11-17:/2016/11/semantic-methods/</id>
    <title type="html">Semantic methods</title>
    <published>2016-11-17T14:00:00Z</published>
    <updated>2016-11-17T14:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2016/11/semantic-methods/"/>
    <content type="html">&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      interleavesym: "|\\mkern-2mu|\\mkern-2mu|",
      interleave: "\\mathrel{\\interleavesym}",
      Interleave: "\\mathop{\\interleavesym}"
    }
  }
});
&lt;/script&gt;

&lt;p&gt;Since CSP is a formal method, it’s not surprising that Roscoe spends a large
part of his textbook talking about how to use rigorous mathematics to talk about
processes.  He actually goes one step (er, two?) further and defines &lt;strong&gt;three
different&lt;/strong&gt; ways to do so:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;denotational semantics&lt;/strong&gt; defines (mathematically, using sets and
sequences) what the &lt;em&gt;behavior&lt;/em&gt; of a process is.  Each CSP operator comes
with a rule for how to calculate a process’s behavior recursively — that is,
in terms of the behavior of its operands.  (So for example, the “external
choice” rule tells you how to define the behavior of \(P \mathrel{\Box}
Q\) in terms of the behavior of \(P\) and \(Q\).)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;algebraic semantics&lt;/strong&gt; tell you to not worry about what a process
“means” or what it “does”.  Instead, it provides a list of &lt;em&gt;rewrite rules&lt;/em&gt;
that let you change what the definition of a process looks like without
changing its behavior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;operational semantics&lt;/strong&gt; say that a process is nothing more than a
state machine, with nodes representing processes (and subprocesses) and
edges representing the events that allow you to transition between them.  We
can learn anything important about a process just by interpreting or
analyzing this state machine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the important contributions of the book is to not just describe these
three different semantic methods (in detail), but to show that they’re
&lt;strong&gt;equivalent&lt;/strong&gt;.  This is great, because all three semantics are useful in
different situations:&lt;/p&gt;</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2016-11-17:/2016/11/refinement-overview/</id>
    <title type="html">Refinement overview</title>
    <published>2016-11-17T13:00:00Z</published>
    <updated>2016-11-17T13:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2016/11/refinement-overview/"/>
    <content type="html">&lt;p&gt;Our goal is to learn about CSP refinement by implementing a refinement checker.
So a good first step is to make sure we’re all on the same page about what
refinement is, and then to step through the refinement algorithm that we mean to
implement.  (If nothing else, that will help make sure I don’t go off on too
many tangents while implementing it!)&lt;/p&gt;

&lt;p&gt;I’ve mentioned refinement elsewhere on this blog a few times (for instance,
&lt;a href="/2016/09/csp-read-atomic-internal/#refinement"&gt;here&lt;/a&gt;).  The basic idea is that
in CSP, you use the same process language to describe the system you’re
designing or investigating, as well as the properties that you would like that
system to have.  (This is unlike most other formal methods, where you have
separate languages for the system and the properties.)  In CSP, the system’s
process is typically called \(Impl\) (for &lt;strong&gt;implementation&lt;/strong&gt;), and the
property description process is typically called \(Spec\) (for
&lt;strong&gt;specification&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;CSP then defines several &lt;strong&gt;semantic models&lt;/strong&gt; that provide rigorous mathematical
definitions of what a process’s behavior is.  You perform a refinement check
within the context of a particular semantic model.  A successful refinement
check tells you that the property defined by \(Spec\) “holds” — specifically,
that all of the behaviors of \(Impl\) are also allowed behaviors of
\(Spec\).  A failed refinement check gives you a &lt;strong&gt;counterexample&lt;/strong&gt; — that is,
a specific behavior of \(Impl\) that was disallowed by \(Spec\).&lt;/p&gt;

&lt;p&gt;The three most common semantic models are &lt;strong&gt;traces&lt;/strong&gt;, &lt;strong&gt;failures&lt;/strong&gt;, and
&lt;strong&gt;failures-divergences&lt;/strong&gt;.  We’ll go into more detail about the mathematics
behind these semantic models in later posts; for now, the 10,000-foot overview
is that:&lt;/p&gt;</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2016-11-16:/2016/11/hst-intro/</id>
    <title type="html">Introduction</title>
    <published>2016-11-16T05:00:00Z</published>
    <updated>2016-11-16T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2016/11/hst-intro/"/>
    <content type="html">&lt;p&gt;It seems like this blog is basically turning into “all things CSP”!  As part of
that trend, I’ve started implementing a new CSP refinement checker called
&lt;a href="https://github.com/hst/hst/"&gt;HST&lt;/a&gt;.  Why do this when there’s a perfectly good refinement checker in
&lt;a href="http://www.cs.ox.ac.uk/projects/fdr/"&gt;FDR&lt;/a&gt;?  Well, I want to learn more about how FDR’s refinement algorithm works.
The algorithm is documented in Bill Roscoe’s &lt;a href="https://www.cs.ox.ac.uk/bill.roscoe/publications/68b.pdf"&gt;textbook&lt;/a&gt; (and a series of
follow-on papers), and working through those descriptions gives you a good bit
of insight into how refinenment really works.  But I often find it easier to
learn a complex topic by implementing it (or at least, by looking at the code of
an implementation).  Hence HST!  In this new series of blog posts, I’m going to
walk through the CSP refinement algorithm in more detail than is presented in
the academic literature, by implementing it (and describing that implementation)
along the way.&lt;/p&gt;

&lt;p&gt;I should emphasize that this is &lt;strong&gt;not&lt;/strong&gt; meant to be a replacement for FDR!  FDR
is a very good piece of software, and if you’re writing any CSP specs in anger,
you probably want FDR at your disposal.  HST is meant to be more of an
educational exercise.  If people find it more generally useful than that, that’s
great!  But it’s not what I’m aiming for.&lt;/p&gt;

&lt;p&gt;(And as for the name, “HST” does &lt;em&gt;not&lt;/em&gt; stand for “Harry S. Truman”, just like
“FDR” does &lt;em&gt;not&lt;/em&gt; stand for “Franklin Delano Roosevelt”.)&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2016-09-07:/2016/09/csp-read-atomic-internal/</id>
    <title type="html">Read Atomic: Internal consistency</title>
    <published>2016-09-07T04:00:00Z</published>
    <updated>2016-09-07T04:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2016/09/csp-read-atomic-internal/"/>
    <content type="html">
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      interleavesym: "|\\mkern-2mu|\\mkern-2mu|",
      interleave: "\\mathrel{\\interleavesym}",
      Interleave: "\\mathop{\\interleavesym}"
    }
  }
});
&lt;/script&gt;

&lt;p&gt;We’ll start by looking at the weakest concurrency model covered in the paper,
Read Atomic.  In fact, to keep things really simple to start with, we’re only
going to look at &lt;em&gt;one&lt;/em&gt; of Read Atomic’s two axioms: &lt;strong&gt;internal consistency&lt;/strong&gt;.
(We’ll look at external consistency in the next post.)&lt;/p&gt;

&lt;!--
 Read Atomic...can be implemented without requiring any coordination among
 replicas...a replica can decide to commit a transaction without consulting
 other replicas.
--&gt;

&lt;p&gt;A transaction is internally consistent if it “reads its own writes”.  This is
the simplest axiom covered in the paper, since it expresses a property that’s
strictly &lt;em&gt;local&lt;/em&gt; to each transaction; we don’t have to consider the behavior of
any other transaction when defining what it means for a transaction to be
internally consistent.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2016-08-03:/2016/08/csp-concurrency-prelims/</id>
    <title type="html">Preliminaries</title>
    <published>2016-08-03T04:00:00Z</published>
    <updated>2016-08-03T04:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2016/08/csp-concurrency-prelims/"/>
    <content type="html">&lt;p&gt;Before diving into the details of our first concurrency model, it will be
helpful to give an overview of how we’re going to use CSP in these posts.&lt;/p&gt;

&lt;p&gt;At the highest level, a CSP specification consists of a set of &lt;em&gt;processes&lt;/em&gt;,
which use &lt;em&gt;events&lt;/em&gt; to describe their behavior.  We use a process to represent
any vantage point that lets us talk about which events occur and in which order.
Each entity in our system is a process (which might be defined using
subprocesses to describe each of its components or parts).  We also use
processes to describe how two or more entities interact with each other, and to
describe any global properties of the entire system.  And lastly, because CSP
verification is based on “refinement”, we also use processes to define the
properties that we hope our system exhibits.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2016-07-28:/2016/07/csp-concurrency-intro/</id>
    <title type="html">Introduction</title>
    <published>2016-07-28T04:00:00Z</published>
    <updated>2016-07-28T04:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2016/07/csp-concurrency-intro/"/>
    <content type="html">&lt;p&gt;Two years ago, I started writing a series of &lt;a href="/2014/01/07/intro-to-csp/"&gt;blog&lt;/a&gt;
&lt;a href="/2014/02/csp-basics/"&gt;posts&lt;/a&gt; about CSP — Communicating Sequential Processes.
That series has…let’s say “stalled”.  Part of the reason is that I didn’t have
a good non-trivial example to work with.  The &lt;a href="https://en.wikipedia.org/wiki/Communicating_sequential_processes#Examples"&gt;stereotypical running
example&lt;/a&gt;
is the vending machine: start with a simple one, which accepts a single coin and
spits out a tea; add more detail as you introduce more of the language.  I
always had a hunch that I needed an example with more meat on it, but could
never find one.&lt;/p&gt;

&lt;p&gt;Fast-forward to today.  I was reading through some of &lt;a href="https://christophermeiklejohn.com/"&gt;Christopher
Meiklejohn’s&lt;/a&gt; work on
&lt;a href="http://lasp-lang.org/"&gt;Lasp&lt;/a&gt;, I came across a citation to a really nice paper
by &lt;a href="http://drops.dagstuhl.de/opus/volltexte/2015/5375/"&gt;Cerone, Bernardi, and
Gotsman&lt;/a&gt;, which adds some
formal rigor to the consistency models that we use to describe modern
distributed systems.  Their formalism is a great combination of simple and
expressive.  The core of the paper is about processes accessing a transactional
data store; the authors provide formal definitions of several concurrency
models, and of some reference implementations that supposedly provide those
concurrency models.  They then use a technique called “observational refinement”
to show that the reference implementations really do provide the concurrency
guarantees in question.&lt;/p&gt;

&lt;p&gt;This approach lines up very well with how you perform refinement checks in CSP
to show that systems satisfy some specification.  And so I finally found my
meaty running example!  I’m resurrecting this blog series, and plan to work
through each of the consistency models and proofs described in the paper,
translating them into CSP processes and refinement checks.  This isn’t an
attempt to replace or outdo anything in the paper!  Far from it — it’s my
attempt to use something more familiar to work through the details of something
less familiar.&lt;/p&gt;

&lt;p&gt;I’m not going to assume a working knowledge of CSP — or of the consistency
models described in the paper!  If you’re familiar with one, my hope is that
you’ll be able to follow along and pick up the other.  And if you’re not
familiar with either…well, I guess we’ll see how good I am at writing an intro
to a difficult topic!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2014-11-21:/2014/11/dependency-management-in-c/</id>
    <title type="html">Dependency management in C</title>
    <published>2014-11-21T05:00:00Z</published>
    <updated>2014-11-21T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2014/11/dependency-management-in-c/"/>
    <content type="html">&lt;p&gt;At &lt;a href="http://www.redjack.com/"&gt;RedJack&lt;/a&gt;, all of our &lt;a href="http://www.redjack.com/solutions/"&gt;core
products&lt;/a&gt; depend on a network sensor that
collects various bits of information about the raw traffic that we see on the
network.  We’re doing some non-trivial analysis on fairly large network links
using commodity hardware, so we’ve implemented this sensor in C.  At its core is
an extremely fast custom &lt;a href="http://en.wikipedia.org/wiki/Flow-based_programming"&gt;flow-based
programming&lt;/a&gt; framework.
It’s a damn cool piece of code, but this post isn’t about the code itself; it’s
about how we deliver that code to our customers.&lt;/p&gt;

&lt;p&gt;Just because we’ve written this component in C, that doesn’t mean we want to
turn our back on the kinds of tooling you get to use when working in other, more
modern languages.  In particular, once you’ve gotten used to modern package
managers like &lt;a href="https://www.npmjs.org/"&gt;npm&lt;/a&gt;, &lt;a href="http://leiningen.org/"&gt;leiningen&lt;/a&gt;,
&lt;a href="http://golang.org/doc/articles/go_command.html"&gt;go&lt;/a&gt;, and
&lt;a href="http://doc.crates.io/guide.html"&gt;Cargo&lt;/a&gt;, it’s hard to go back to things like
&lt;a href="http://www.cmake.org/"&gt;CMake&lt;/a&gt; and &lt;em&gt;[shudder]&lt;/em&gt; the
&lt;a href="http://en.wikipedia.org/wiki/GNU_build_system"&gt;autotools&lt;/a&gt;.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2014-05-14:/2014/05/git-flow/</id>
    <title type="html">Tagged releases using git flow</title>
    <published>2014-05-14T04:00:00Z</published>
    <updated>2014-05-14T04:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2014/05/git-flow/"/>
    <content type="html">&lt;p&gt;I’ve used &lt;code&gt;git flow&lt;/code&gt; for most of my software projects — specifically for those
that have versioned releases.  The &lt;a href="http://nvie.com/posts/a-successful-git-branching-model/"&gt;original
post&lt;/a&gt; describing &lt;code&gt;git
flow&lt;/code&gt; is still the best overview of how it works.  In short, you have a &lt;code&gt;master&lt;/code&gt;
branch where &lt;strong&gt;every&lt;/strong&gt; commit is a merge commit.  Each of these merge commits
represents a new release, and is tagged with the version number of that release.
The merge brings in all of the subsidiary commits and feature branches that make
up that release.  Ongoing work happens on a separate &lt;code&gt;develop&lt;/code&gt; branch.  This is
where you merge in completed new features and bug fixes on a day-to-day basis.
&lt;code&gt;develop&lt;/code&gt; should always be a stable version of the software — you don’t merge a
feature branch into &lt;code&gt;develop&lt;/code&gt; until it passes all of your tests and is
“complete” with regards to the feature you’re trying to implement.&lt;/p&gt;

&lt;p&gt;My favorite part of this model is how each release is just some tagged commit on
the &lt;code&gt;master&lt;/code&gt; branch.  You want to see the code for the latest released version?
That’s easy — &lt;code&gt;git checkout master&lt;/code&gt;.  You want version 1.2.5 specifically?  Use
&lt;code&gt;git checkout 1.2.5&lt;/code&gt; instead.&lt;/p&gt;

&lt;p&gt;Unfortunately, the &lt;a href="https://github.com/nvie/gitflow"&gt;&lt;code&gt;git flow&lt;/code&gt; tool&lt;/a&gt; has
implemented a &lt;a href="https://github.com/nvie/gitflow/issues/206"&gt;slightly different
behavior&lt;/a&gt; for awhile now.  That
patch makes &lt;code&gt;git flow&lt;/code&gt; tag the last commit on the release branch, instead of the
merge commit on the &lt;code&gt;master&lt;/code&gt; branch.  The reasons for this might be perfectly
valid, but it’s not what I want, and it’s not what the original &lt;code&gt;git flow&lt;/code&gt; post
described.  That means that I can’t use &lt;code&gt;git flow release finish&lt;/code&gt; as-is.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2014-02-28:/2014/02/csp-basics/</id>
    <title type="html">CSP: The basics</title>
    <published>2014-02-28T05:00:00Z</published>
    <updated>2014-02-28T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2014/02/csp-basics/"/>
    <content type="html">&lt;p class="big-def"&gt;&lt;em class="tldr"&gt;tl;dr&lt;/em&gt; CSP is a &lt;em&gt;formal method&lt;/em&gt; that lets you describe and reason
about the behavior of &lt;em&gt;concurrent systems&lt;/em&gt;.  CSP is &lt;em&gt;composable&lt;/em&gt;; you write
simple &lt;em&gt;processes&lt;/em&gt;, and then use special &lt;em&gt;operators&lt;/em&gt; to combine them together
into larger, more complex processes.  A process is a summary of some system; it
uses &lt;em&gt;events&lt;/em&gt; to describe how that system works, and to &lt;em&gt;synchronously
communicate&lt;/em&gt; with other processes.  You can compare two processes using a
&lt;em&gt;refinement check&lt;/em&gt;; this lets us check, for instance, whether a real-world
system satisfies some important safety or liveness property.  CSP has good &lt;em&gt;tool
support&lt;/em&gt;, which lets us perform these refinement checks quickly and
automatically.&lt;/p&gt;

&lt;p&gt;Well that was easy, wasn’t it?  You can boil just about anything down to a
single paragraph.  Let’s look at each of those key points in more detail.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2014-01-07:/2014/01/07/intro-to-csp/</id>
    <title type="html">CSP: An introduction</title>
    <published>2014-01-07T05:00:00Z</published>
    <updated>2014-01-07T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2014/01/07/intro-to-csp/"/>
    <content type="html">&lt;p&gt;Communicating Sequential Processes (CSP) has been around for almost four decades
at this point, but for much of its life, it was only well-known among
theoretical computer scientists and formal methods advocates.  More recently,
many more people have at least &lt;em&gt;heard&lt;/em&gt; of CSP, largely because it inspired the
&lt;a href="http://golang.org/doc/effective_go.html#concurrency"&gt;concurrency support&lt;/a&gt; in
&lt;a href="http://golang.org/"&gt;Go&lt;/a&gt;, a popular mainstream programming language.  However,
if you ask most people what it &lt;em&gt;means&lt;/em&gt; to be inspired by CSP, the most common
response would probably be “erm, something about message passing”?&lt;/p&gt;

&lt;p&gt;That said, CSP isn’t just some dusty theory that inspired part of Go; it can
also help us understand the distributed systems that we create.  We’ve developed
a &lt;a href="http://zookeeper.apache.org/"&gt;plethora&lt;/a&gt; &lt;a href="http://www.mongodb.org/"&gt;of&lt;/a&gt;
&lt;a href="http://couchdb.apache.org/"&gt;tools&lt;/a&gt; &lt;a href="http://redis.io/"&gt;that&lt;/a&gt;
&lt;a href="http://basho.com/riak/"&gt;help&lt;/a&gt; &lt;a href="http://cassandra.apache.org/"&gt;us&lt;/a&gt;
&lt;a href="http://hbase.apache.org/"&gt;build&lt;/a&gt; &lt;a href="http://hadoop.apache.org/"&gt;distributed&lt;/a&gt;
&lt;a href="http://storm-project.net/"&gt;systems&lt;/a&gt;.  But unfortunately, we don’t always
understand of how those tools work, how they fail, and how they interact when we
piece them together into a larger system.  We can all name-drop the &lt;a href="http://dl.acm.org/citation.cfm?id=564601"&gt;CAP
theorem&lt;/a&gt;, but do you &lt;em&gt;really&lt;/em&gt; know
what your system is going to do when the network partitions, or when a host
dies?  How do you convince someone that you’re right?&lt;/p&gt;

&lt;p&gt;We can’t just rely on intuition and hand-wavy arguments; our systems are too
large, and too important, for that.  So how do you address these concerns with
rigor?  There are two main approaches: you can either &lt;em&gt;test&lt;/em&gt; your assumptions
empirically on a running system, or you can describe your system in detail and
&lt;em&gt;prove&lt;/em&gt; that your assumptions are correct.  Kyle Kingsbury has great examples of
both: &lt;a href="http://aphyr.com/tags/jepsen"&gt;Jepsen&lt;/a&gt; on the testing side,
&lt;a href="http://aphyr.com/posts/309-knossos-redis-and-linearizability"&gt;Knossos&lt;/a&gt; on the
proof side.  Both approaches are important; if you want to be convincing, you
have to choose at least one of them.  If you prefer the proof-based approach,
CSP is another option.  If you only think of CSP in terms of Go’s concurrency
primitives, or if you haven’t thought of it at all, then you overlook the fact
that CSP was &lt;em&gt;specifically designed&lt;/em&gt; to help answer these kinds of questions.&lt;/p&gt;

&lt;p&gt;In this series of articles, I want to describe how CSP fits into this landscape,
for developers with a range of expertise.  For the every-day programmer, I want
to give a basic, high-level introduction to CSP, and to explain what it means
for Go to be inspired by CSP.  For the distributed systems engineer, I want to
add weight to the argument that formal methods are a useful tool for studying
and designing the systems that we create and use.  And for the formal methodist,
I want to show how to use CSP in particular to specify and reason about those
systems.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-05-13:/2010/05/13/powerpc-qemu-lucid/</id>
    <title type="html">Installing Ubuntu Lucid on a PowerPC QEMU virtual machine</title>
    <published>2010-05-13T04:00:00Z</published>
    <updated>2010-05-13T04:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/05/13/powerpc-qemu-lucid/"/>
    <content type="html">&lt;p&gt;Part of the software I help develop at
&lt;a href="http://www.redjack.com/"&gt;RedJack&lt;/a&gt; needs to be tested on both
little-endian and big-endian machines.  Little-endian machines are
easy, since everyone and their mother is running on a little-endian
Intel or AMD x86 chip.  It used to be that big-endian was pretty easy
to test, too — just break out your trusty Apple Powerbook G4 and
you’re good to go.  Since Apple has shifted over to Intel chips,
though, the situation has changed.&lt;/p&gt;

&lt;p&gt;Luckily, &lt;a href="http://wiki.qemu.org/"&gt;QEMU&lt;/a&gt; has PowerPC as one of the
targets that it can emulate, so in theory, I can still easily test my
code on a big-endian machine by creating a QEMU PowerPC virtual
machine.  There’s already a writeup about trying to install Debian
onto a QEMU VM
&lt;a href="http://machine-cycle.blogspot.com/2009/05/running-debian-on-qemu-powerpc.html"&gt;here&lt;/a&gt;.
&lt;a href="http://www.aurel32.net/"&gt;Aurélien Jarno&lt;/a&gt; has graciously put together
downloadable disk images with Debian preinstalled.  If that’s good
enough for your purposes, just go download those!  You won’t need any
of the rest of the information on this page.&lt;/p&gt;

&lt;p&gt;Unfortunately, I didn’t want to run stock Debian; my little-endian
build machine is running Ubuntu Lucid, and for consistency, I wanted
my big-endian VM to be running the same.  As it turns out, this also
required a fair dose of masochism on my part.  There are several
issues that you’ll encounter if you try to do this by hand.  Here is
my cheat sheet for getting around these issues.&lt;/p&gt;

&lt;p&gt;Note that this isn’t a full step-by-step account of how to install
Lucid onto a QEMU VM.  For now, I’m just trying to get my notes down
into a more permanent form.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-02-25:/2010/02/25/libpush-callbacks-part-1/</id>
    <title type="html">Parser callbacks in libpush, Part 1 — Streams</title>
    <published>2010-02-25T05:00:00Z</published>
    <updated>2010-02-25T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/02/25/libpush-callbacks-part-1/"/>
    <content type="html">&lt;p&gt;This post is the first in a series that describes the
&lt;code&gt;push_callback_t&lt;/code&gt; type in the
&lt;a href="http://github.com/dcreager/libpush/"&gt;libpush&lt;/a&gt; library.  In these
posts, we’ll walk through a couple of possible ways to implement
callbacks under the covers.  At each stage, we’ll encounter problems
with the current design.  Fixing these problems should lead closer us
to the actual implementation in libpush, and along the way, we’ll gain
a good understanding of how our design decisions affect the
performance and usability of the library.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;push_callback_t&lt;/code&gt; type is used to define &lt;em&gt;parser callbacks&lt;/em&gt;, which
are the basic unit of parsing in libpush.  Callbacks are pretty
simple: they take in an &lt;em&gt;input value&lt;/em&gt;, read some data from the &lt;em&gt;input
stream&lt;/em&gt;, and produce an &lt;em&gt;output value&lt;/em&gt;.  (The fact that callbacks take
in an input value, in addition to reading from the input stream, is
what makes them &lt;a href="http://www.haskell.org/arrows/"&gt;&lt;em&gt;arrows&lt;/em&gt;&lt;/a&gt; instead of
&lt;a href="http://en.wikipedia.org/wiki/Monad_%28functional_programming%29"&gt;&lt;em&gt;monads&lt;/em&gt;&lt;/a&gt;
— but that’s a story for a later post).&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-02-17:/2010/02/17/llvm-lto-karmic/</id>
    <title type="html">Using LLVM's link-time optimization on Ubuntu Karmic</title>
    <published>2010-02-17T05:00:00Z</published>
    <updated>2010-02-17T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/02/17/llvm-lto-karmic/"/>
    <content type="html">&lt;p&gt;While playing around with
&lt;a href="http://github.com/dcreager/libpush"&gt;libpush&lt;/a&gt; on my MacBook, I was
pleasantly surprised to see a huge performance increase when I used
the link-time optimization (LTO) feature of the LLVM GCC front end.
(It’s really quite nifty; the new &lt;a href="http://github.com/mxcl/homebrew"&gt;Homebrew package
manager&lt;/a&gt; uses it by default when
compiling packages.)  On MacOS, using LTO is as simple as using
&lt;code&gt;llvm-gcc&lt;/code&gt; as your C compiler (or &lt;code&gt;llvm-g++&lt;/code&gt; if you’re compiling C++),
and passing in &lt;code&gt;-O4&lt;/code&gt; as your optimization flag.  I use SCons as my
builder, so this turns into:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scons CC=llvm-gcc CCFLAGS=-O4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will cause GCC to output LLVM bytecode into the &lt;em&gt;.o&lt;/em&gt; output
files, and to perform whole-program optimizations during each linking
phase.  I was able to see a big performance win simply from the linker
being able to inline in copies of small functions that live in “other”
compilation units.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-02-10:/2010/02/10/setuptools-git-version-numbers/</id>
    <title type="html">Extracting setuptools version numbers from your git repository</title>
    <published>2010-02-10T05:00:00Z</published>
    <updated>2010-02-10T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/02/10/setuptools-git-version-numbers/"/>
    <content type="html">&lt;p&gt;Just like everyone else, we’re using
&lt;a href="http://pypi.python.org/pypi/setuptools"&gt;setuptools&lt;/a&gt; as the core of
the build system for our Python-based projects.  For the most part,
this has been a painless, straightforward process.  However, one
lingering annoyance is that we’ve been specifying the version number
directly in our &lt;em&gt;setup.py&lt;/em&gt; files:&lt;/p&gt;

&lt;pre class="CodeRay"&gt;&lt;code&gt;&lt;span class="keyword"&gt;from&lt;/span&gt; &lt;span class="include"&gt;setuptools&lt;/span&gt; &lt;span class="keyword"&gt;import&lt;/span&gt; &lt;span class="include"&gt;setup&lt;/span&gt;

setup(
    name = &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;awesomelib&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;,
    version = &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;1.2&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="comment"&gt;# ...etc&lt;/span&gt;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On our maintenance branches, we get a nice &lt;em&gt;awesomelib-1.2.tar.gz&lt;/em&gt;
file when we run &lt;code&gt;python setup.py sdist&lt;/code&gt;.  On our development branch,
we’ve also got the following &lt;em&gt;setup.cfg&lt;/em&gt; file:&lt;/p&gt;

&lt;pre class="CodeRay"&gt;&lt;code&gt;[egg_info]
tag_build = dev
tag_date = true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That gives us tarballs like &lt;em&gt;awesomelib-1.2dev-20100210.tar.gz&lt;/em&gt; on our
development branch.  Because we’re using the &lt;code&gt;dev&lt;/code&gt; suffix, which
setuptools considers to be a “prerelease”, we have to remember to
increment the version number in development whenever we cut a new
release.  The end result is that we have a longish process for
creating releases.  If we want to create a new 1.3 release, we have to
do the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a new maintenance branch for 1.3:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;$ git checkout -b maint-1.3 master
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Update the &lt;em&gt;setup.cfg&lt;/em&gt; file to remove the &lt;code&gt;tag_build&lt;/code&gt; and
&lt;code&gt;tag_date&lt;/code&gt; entries.  Commit this with a “Tagging version 1.3”
commit message.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Back on the development branch, update &lt;em&gt;setup.py&lt;/em&gt; to increment the
“development version” to 1.4.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Granted, this isn’t horribly difficult, but we can do better.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-02-06:/2010/02/06/libpush/</id>
    <title type="html">A combinator-based parsing library for C</title>
    <published>2010-02-06T05:00:00Z</published>
    <updated>2010-02-06T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/02/06/libpush/"/>
    <content type="html">&lt;p&gt;Recently I’ve been working on
&lt;a href="http://github.com/dcreager/libpush/"&gt;libpush&lt;/a&gt;, which a new parsing
library for C.  It has two main features that I think will be
valuable: it’s a &lt;em&gt;push parser&lt;/em&gt;, which means that instead of parsing a
file, stream, or single memory buffer, you supply the data (or “push”
it) to the parser in chunks, as it becomes available.  I plan to
discuss this aspect of the parser in more detail in a later post.&lt;/p&gt;

&lt;p&gt;The other main feature is that you design your parsers using
&lt;em&gt;combinators&lt;/em&gt;.  Parser combinators are widely used in Haskell, with
&lt;a href="http://legacy.cs.uu.nl/daan/parsec.html"&gt;Parsec&lt;/a&gt; being the most
common example.  Combinator-based parsing libraries are especially
nice in Haskell, because Haskell’s syntax makes them look very simple.
For instance, a parser that parses matching nested parentheses is:&lt;/p&gt;

&lt;pre class="CodeRay"&gt;&lt;code&gt;parens :: Parser ()
parens = (char '(' &amp;gt;&amp;gt; parens &amp;gt;&amp;gt; char ')' &amp;gt;&amp;gt; parens) &amp;lt;|&amp;gt; return ()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, the &lt;code&gt;&amp;lt;|&amp;gt;&lt;/code&gt; operator represents &lt;em&gt;choice&lt;/em&gt;: we try parsing the left
operand, and if it fails, then we try the right operand.  In our
example, the right operand is the base case, which matches the empty
string.  The left operand parses an opening parenthesis; then
recursively calls itself to match any parentheses that might be nested
in the current set; then parses the closing parenthesis; and then
finally tries to match a nested set that occurs after the current set.&lt;/p&gt;</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-02-05:/2010/02/05/omnigraffle-5-export/</id>
    <title type="html">Updating graffle-export to work with OmniGraffle 5</title>
    <published>2010-02-05T05:00:00Z</published>
    <updated>2010-02-05T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/02/05/omnigraffle-5-export/"/>
    <content type="html">&lt;p&gt;I recently upgraded to OmniGraffle 5, which caused my
&lt;a href="http://github.com/dcreager/graffle-export/"&gt;graffle-export&lt;/a&gt; script to
break:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ graffle.sh ~/git/cwa/figures/analyst.graffle foo.pdf 
OmniGraffle Professional 5
/Users/dcreager/git/cwa/figures/analyst.graffle
./graffle.scpt: execution error: OmniGraffle Professional 5 got an error: The document cannot be exported to the "pdf" format. (-50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(This was first reported to me by Nima Talebi as &lt;a href="http://github.com/dcreager/graffle-export/issues/issue/1"&gt;a
ticket&lt;/a&gt; on
graffle-export’s Github page.)&lt;/p&gt;

&lt;p&gt;Before we can understand what error we’re seeing, a little explanation
is in order.  The core logic of the OmniGraffle exporter is an
AppleScript.  Unfortunately, AppleScripts are stored in a binary
format, so if you go to the Github page, you can’t easily view the
contents of the file.  The important line of the script is:&lt;/p&gt;</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-01-08:/2010/01/08/default-scons-clean-targets/</id>
    <title type="html">Default “scons -c” targets</title>
    <published>2010-01-08T05:00:00Z</published>
    <updated>2010-01-08T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/01/08/default-scons-clean-targets/"/>
    <content type="html">&lt;p&gt;As I mentioned in a &lt;a href="/2009/12/18/make-distclean-in-scons/"&gt;previous
post&lt;/a&gt;, the automatic “clean”
target provided by SCons (&lt;code&gt;scons -c&lt;/code&gt;) is very useful for cleaning out
build files, without requiring much in the way of configuration.
Anything that SCons generates when you run &lt;code&gt;scons&lt;/code&gt; will be
automatically cleaned when you run &lt;code&gt;scons -c&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;While useful, I’d like more control over the behavior of &lt;code&gt;scons -c&lt;/code&gt;.
Specifically, being a good TDD junkie, I have several test cases that
I can run using &lt;code&gt;scons test&lt;/code&gt;:&lt;/p&gt;

&lt;pre class="CodeRay"&gt;&lt;code&gt;build_test = env.Program( ... )
env.Alias(&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;build-tests&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, build_test)

run_test = env.Alias(&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, [build_test],
                     [&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;@%s&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; % build_test[&lt;span class="integer"&gt;0&lt;/span&gt;].abspath])
env.AlwaysBuild(run_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By setting it up this way, the test programs aren’t built by default:
you have to explicitly run &lt;code&gt;scons build-tests&lt;/code&gt; (if you want to build
the tests but not run them) or &lt;code&gt;scons test&lt;/code&gt; (if you want to build and
run them).  Moreover, because of SCons’s dependency tracking, I can
just use &lt;code&gt;scons test&lt;/code&gt; as my usual build command during my
Edit-Test-Debug loop.  SCons will automatically rebuild any changed
source files before running the tests.&lt;/p&gt;

&lt;p&gt;All of this is great.  So what’s the problem?  As I mentioned above,
&lt;code&gt;scons -c&lt;/code&gt; only cleans the build files that are created by &lt;code&gt;scons&lt;/code&gt; —
and since I’ve explicitly set things up so that tests aren’t &lt;em&gt;built&lt;/em&gt;
by default, they’ll also not be &lt;em&gt;cleaned&lt;/em&gt; by default.  This means that
to fully clean out my build targets, I have to run two commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scons -c
$ scons -c build-tests
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not ideal!  I’d prefer if &lt;code&gt;scons -c&lt;/code&gt; cleaned everything, just like
&lt;code&gt;make clean&lt;/code&gt; would in the Automake world.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2010-01-05:/2010/01/05/omnigraffle-export/</id>
    <title type="html">Exporting OmniGraffle documents from the command line</title>
    <published>2010-01-05T05:00:00Z</published>
    <updated>2010-01-05T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2010/01/05/omnigraffle-export/"/>
    <content type="html">&lt;p&gt;&lt;a href="http://www.omnigroup.com/applications/OmniGraffle/"&gt;OmniGraffle&lt;/a&gt; is
my tool of choice for creating figures for my papers.  It’s biggest
drawback is that it’s only available for Mac OS, which can make it
cumbersome if I’m working on one of my Linux machines and need to
create or modify a figure.  But it’s ease-of-use and the quality of
the figures it creates are hard to beat.&lt;/p&gt;

&lt;p&gt;Of course, creating the figure isn’t enough — since I write my papers
in LaTeX, I have to export my figures into EPS or PDF (depending on
whether I’m creating a PostScript or PDF version of the paper) before
I can use them in my documents.  It’s easy enough to use the Export
dialog to do this (keyboard shortcut: ⌥⌘E), but ideally I’d like the
ability to export figures from the command line.  Coupled with a good
Makefile, this would let me run a simple &lt;code&gt;make paper&lt;/code&gt; command, and
automatically re-export any necessary figures before rebuilding the
paper itself.&lt;/p&gt;

&lt;p&gt;Luckily, OmniGraffle has always had rather good support for being
controlled via AppleScript.  The commands can be somewhat
undocumented, requiring a bit of trial and error, but while entrenched
in our PhD studies at Oxford, my colleague David Faitelson and I were
able to whip together a script that suited our needs.  I’ve recently
extracted the code from our Oxford SVN repository and uploaded it to
&lt;a href="http://github.com/dcreager/graffle-export"&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To install the script, just place the &lt;em&gt;graffle.sh&lt;/em&gt; and &lt;em&gt;graffle.scpt&lt;/em&gt;
files into some directory that’s on your &lt;code&gt;$PATH&lt;/code&gt;, such as
&lt;em&gt;/usr/local/bin&lt;/em&gt; or &lt;em&gt;$HOME/bin&lt;/em&gt;.  Then just run&lt;/p&gt;</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2009-12-23:/2009/12/23/high-water-mark-buffers/</id>
    <title type="html">“High-water mark” buffers</title>
    <published>2009-12-23T05:00:00Z</published>
    <updated>2009-12-23T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2009/12/23/high-water-mark-buffers/"/>
    <content type="html">&lt;p&gt;My coding project for today was to extract out some code for dealing
with “high-water mark buffers”, putting it in a separate library call
&lt;code&gt;libhwm&lt;/code&gt;.  In this post, I’m going to describe the rationale for using
them, and a brief overview of how to use the library.  (The library is
hosted on &lt;a href="http://github.com/dcreager/libhwm/"&gt;Github&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;By the way, this post (and the library) is all in C.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <id>tag:dcreager.net,2009-12-21:/2009/12/21/decentralized-datatypes/</id>
    <title type="html">Decentralized datatypes</title>
    <published>2009-12-21T05:00:00Z</published>
    <updated>2009-12-21T05:00:00Z</updated>
    <link rel="alternate" href="http://dcreager.net/2009/12/21/decentralized-datatypes/"/>
    <content type="html">&lt;p&gt;Over the past year or so there have been quite a few blog postings in
the REST world about MIME types, and their role in the REST
architecture.  A lot of the discussion seems to be prompted by WADL,
which is an attempt to define a WSDL-style interface description
language for REST services.  &lt;a href="http://bitworking.org/news/193/Do-we-need-WADL"&gt;Joe
Gregorio&lt;/a&gt; argues that
MIME types are more useful for describing the semantics of a service
than a WADL document, since there are parts of the service’s semantics
that just can’t be encoded in a machine-readable format.  MIME types
acknowledge this, providing a standard way of identifying a data
format and pointing to the human- and machine-readable documents (such
as RFCs and XSDs) that define the syntax and accompanying semantics.&lt;/p&gt;

&lt;p&gt;Following this idea, several people have begun debating whether or not
the centralized assignment of MIME types is the right way to handle
the variety of data formats that REST-based systems produce and
consume.  &lt;a href="http://www.markbaker.ca/blog/2008/02/media-type-centralization-is-a-feature-not-a-bug/"&gt;Mark
Baker&lt;/a&gt;
comes in on the side of centralized assignment, whereas &lt;a href="http://www.innoq.com/blog/st/2008/02/decentralizing_media_types.html"&gt;Stefan
Tilkov&lt;/a&gt;,
&lt;a href="http://netzooid.com/blog/2008/02/07/why-a-restful-idl-is-an-oxymoron-and-what-we-really-need-instead/"&gt;Dan
Diephouse&lt;/a&gt;,
and &lt;a href="http://macstrac.blogspot.com/2007/11/atompub-services-and-auto-detecting.html"&gt;James
Strachan&lt;/a&gt;
argue in favor of decentralized types.  &lt;a href="http://bill.burkecentral.com/2008/03/05/restful-xml-content-negotitation/"&gt;Bill
Burke&lt;/a&gt;
and &lt;a href="http://soundadvice.id.au/blog/2009/08/16/#mimeLimitation"&gt;Benjamin
Carlyle&lt;/a&gt;
have good summaries of the different proposed technical solutions that
would enable decentralized types.&lt;/p&gt;

</content>
  </entry>
</feed>
